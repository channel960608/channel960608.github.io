<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta name="baidu-site-verification" content="zhTMODK6kb" />
  <meta http-equiv="content-language" content="zh-cn">
  
  <title>通过20newsgroups案例进行Sklearn入门学习 | Coder Am I</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="通过20newsgroups案例进行Sklearn入门学习为了对sklearn进行学习，在下将20newsgroups案例作为文本分析入手项目，并参考这篇博客完成了代码的编写并编译通过。下面进一步分析代码。   流程 加载数据集 提取feature 分类 Naive Beyes KNN SVM   聚类    加载数据集数据集下载20news-19997.tar.gz,下载后解压到根目录的~/sc">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="通过20newsgroups案例进行Sklearn入门学习">
<meta property="og:url" content="http://channel960608.github.io/2018/04/11/通过20newsgroups案例进行Sklearn入门学习/index.html">
<meta property="og:site_name" content="Coder Am I">
<meta property="og:description" content="通过20newsgroups案例进行Sklearn入门学习为了对sklearn进行学习，在下将20newsgroups案例作为文本分析入手项目，并参考这篇博客完成了代码的编写并编译通过。下面进一步分析代码。   流程 加载数据集 提取feature 分类 Naive Beyes KNN SVM   聚类    加载数据集数据集下载20news-19997.tar.gz,下载后解压到根目录的~/sc">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-04-11T12:09:15.864Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="通过20newsgroups案例进行Sklearn入门学习">
<meta name="twitter:description" content="通过20newsgroups案例进行Sklearn入门学习为了对sklearn进行学习，在下将20newsgroups案例作为文本分析入手项目，并参考这篇博客完成了代码的编写并编译通过。下面进一步分析代码。   流程 加载数据集 提取feature 分类 Naive Beyes KNN SVM   聚类    加载数据集数据集下载20news-19997.tar.gz,下载后解压到根目录的~/sc">
  
  
    <link rel="icon" href="/favicon.png">
  
  <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,600' rel='stylesheet' type='text/css'> -->
  <!-- <link href="//fonts.googleapis.com/css?family=Source+Code+Pro:400,700" rel="stylesheet" type="text/css"> -->
  <!--<link href="//fonts.useso.com/css?family=Source+Code+Pro:400,700" rel="stylesheet" type="text/css">-->
  <!--<link href='//fonts.useso.com/css?family=Open+Sans:300,600' rel='stylesheet' type='text/css'>-->
  <link href="/css/fonts/font.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <a href="/" class="logo">Coder Am I</a>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about/index.html">About</a>
        
          <a class="main-nav-link" href="/board">Board</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://channel960608.github.io"></form>
        </div>
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
        
        
          <a id="nav-github-link" class="nav-icon" href="https://github.com/channel960608" title="Fork me on GitHub"></a>
        
      </nav>
    </div>
  </div>
</header>
      <nav id="mobile-nav" class="off">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
    <a href="/board" class="mobile-nav-link">Board</a>
  
  <div id="search-form-wrap-mobile">
    <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://channel960608.github.io"></form>
  </div>
</nav>
      <div class="outer">
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/12/13/寻找陈络月/">Search for Chenluo Month</a>
          </li>
        
          <li>
            <a href="/2018/04/11/通过20newsgroups案例进行Sklearn入门学习/">通过20newsgroups案例进行Sklearn入门学习</a>
          </li>
        
          <li>
            <a href="/2018/04/08/k近邻算法中kd树的实现/">k近邻算法中kd树的实现</a>
          </li>
        
          <li>
            <a href="/2018/03/19/一致性hash算法/">一致性hash算法原理简要理解</a>
          </li>
        
          <li>
            <a href="/2017/07/21/航天航空与臭虫/">航天航空与臭虫</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CORBA/">CORBA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java-Web/">Java Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tomcat-Linux/">Tomcat && Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/其他/">其他</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式数据库/">分布式数据库</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CORBA/" style="font-size: 10px;">CORBA</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Java-Web/" style="font-size: 10px;">Java Web</a> <a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Tomcat-Linux/" style="font-size: 10px;">Tomcat && Linux</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a> <a href="/tags/分布式数据库/" style="font-size: 10px;">分布式数据库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://hexo.io">Hexo</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
        <section id="main"><article id="post-通过20newsgroups案例进行Sklearn入门学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/11/通过20newsgroups案例进行Sklearn入门学习/" class="article-date">
  <time datetime="2018-04-11T10:29:55.000Z" itemprop="datePublished">2018-04-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      通过20newsgroups案例进行Sklearn入门学习
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <div id="toc" class="toc-article">
          <strong class="toc-title">文章目录</strong>
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#通过20newsgroups案例进行Sklearn入门学习"><span class="toc-number">1.</span> <span class="toc-text"><a href="#&#x901A;&#x8FC7;20newsgroups&#x6848;&#x4F8B;&#x8FDB;&#x884C;Sklearn&#x5165;&#x95E8;&#x5B66;&#x4E60;" class="headerlink" title="&#x901A;&#x8FC7;20newsgroups&#x6848;&#x4F8B;&#x8FDB;&#x884C;Sklearn&#x5165;&#x95E8;&#x5B66;&#x4E60;"></a>&#x901A;&#x8FC7;20newsgroups&#x6848;&#x4F8B;&#x8FDB;&#x884C;Sklearn&#x5165;&#x95E8;&#x5B66;&#x4E60;</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#流程"><span class="toc-number">1.0.1.</span> <span class="toc-text"><a href="#&#x6D41;&#x7A0B;" class="headerlink" title="&#x6D41;&#x7A0B;"></a>&#x6D41;&#x7A0B;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加载数据集"><span class="toc-number">1.0.2.</span> <span class="toc-text"><a href="#&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x96C6;" class="headerlink" title="&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x96C6;"></a>&#x52A0;&#x8F7D;&#x6570;&#x636E;&#x96C6;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提取feature"><span class="toc-number">1.0.3.</span> <span class="toc-text"><a href="#&#x63D0;&#x53D6;feature" class="headerlink" title="&#x63D0;&#x53D6;feature"></a>&#x63D0;&#x53D6;feature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类训练"><span class="toc-number">1.0.4.</span> <span class="toc-text"><a href="#&#x5206;&#x7C7B;&#x8BAD;&#x7EC3;" class="headerlink" title="&#x5206;&#x7C7B;&#x8BAD;&#x7EC3;"></a>&#x5206;&#x7C7B;&#x8BAD;&#x7EC3;</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Naive-Bayes-朴素贝叶斯法"><span class="toc-number">1.0.4.0.1.</span> <span class="toc-text"><a href="#Naive-Bayes-&#x6734;&#x7D20;&#x8D1D;&#x53F6;&#x65AF;&#x6CD5;" class="headerlink" title="Naive Bayes  &#x6734;&#x7D20;&#x8D1D;&#x53F6;&#x65AF;&#x6CD5;"></a>Naive Bayes  &#x6734;&#x7D20;&#x8D1D;&#x53F6;&#x65AF;&#x6CD5;</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#KNN-k近邻算法"><span class="toc-number">1.0.4.0.2.</span> <span class="toc-text"><a href="#KNN-k&#x8FD1;&#x90BB;&#x7B97;&#x6CD5;" class="headerlink" title="KNN  k&#x8FD1;&#x90BB;&#x7B97;&#x6CD5;"></a>KNN  k&#x8FD1;&#x90BB;&#x7B97;&#x6CD5;</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SVM-支持向量机"><span class="toc-number">1.0.4.0.3.</span> <span class="toc-text"><a href="#SVM-&#x652F;&#x6301;&#x5411;&#x91CF;&#x673A;" class="headerlink" title="SVM  &#x652F;&#x6301;&#x5411;&#x91CF;&#x673A;"></a>SVM  &#x652F;&#x6301;&#x5411;&#x91CF;&#x673A;</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#聚类"><span class="toc-number">1.0.5.</span> <span class="toc-text"><a href="#&#x805A;&#x7C7B;" class="headerlink" title="&#x805A;&#x7C7B;"></a>&#x805A;&#x7C7B;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估"><span class="toc-number">1.0.6.</span> <span class="toc-text"><a href="#&#x6A21;&#x578B;&#x8BC4;&#x4F30;" class="headerlink" title="&#x6A21;&#x578B;&#x8BC4;&#x4F30;"></a>&#x6A21;&#x578B;&#x8BC4;&#x4F30;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#小结"><span class="toc-number">1.0.7.</span> <span class="toc-text"><a href="#&#x5C0F;&#x7ED3;" class="headerlink" title="&#x5C0F;&#x7ED3;"></a>&#x5C0F;&#x7ED3;</span></a></li></ol></li></ol></li></ol>
        </div>
        

        <h1 id="通过20newsgroups案例进行Sklearn入门学习"><a href="#通过20newsgroups案例进行Sklearn入门学习" class="headerlink" title="通过20newsgroups案例进行Sklearn入门学习"></a>通过20newsgroups案例进行Sklearn入门学习</h1><p>为了对sklearn进行学习，在下将20newsgroups案例作为文本分析入手项目，并参考这篇<a href="https://blog.csdn.net/abcjennifer/article/details/23615947" target="_blank" rel="noopener">博客</a>完成了代码的编写并编译通过。下面进一步分析代码。  </p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ul>
<li>加载数据集</li>
<li>提取feature</li>
<li>分类<ul>
<li>Naive Beyes</li>
<li>KNN</li>
<li>SVM</li>
</ul>
</li>
<li>聚类  </li>
</ul>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>数据集下载<a href="http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz" target="_blank" rel="noopener">20news-19997.tar.gz</a>,下载后解压到根目录的<code>~/scikit_learn_data/</code>目录下，这个是数据默认路径。    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line">ategories = [<span class="string">'comp.graphics'</span>,</span><br><span class="line">    <span class="string">'comp.os.ms-windows.misc'</span>,</span><br><span class="line">    <span class="string">'comp.sys.ibm.pc.hardware'</span>,</span><br><span class="line">    <span class="string">'comp.sys.mac.hardware'</span>,</span><br><span class="line">    <span class="string">'comp.windows.x'</span>]</span><br><span class="line">newsgroup_train = fetch_20newsgroups(subset = <span class="string">'train'</span>, categories = categories)</span><br><span class="line">newsgroup_test = fetch_20newsgroups(subset = <span class="string">'test'</span>, categories = categories)</span><br></pre></td></tr></table></figure>
<p>对于<code>fetch_20newsgroups()</code>这个方法：<br><code>fetch_20newsgroups(data_home=None,subset=&#39;train&#39;,categories=None,shuffle=True,random_state=42,remove=(),download_if_missing=True)</code>  </p>
<ul>
<li><code>data_home</code> 数据集的地址，默认为<code>~/scikit_learn_data/</code>目录</li>
<li><code>subset</code> {train, test, all}三者可选，分别对应训练集，测试集，所有样本</li>
<li><code>categories</code> 类别，指定类别，可以提取出指定的目标类；如果是默认，则提取所有数据  </li>
<li><code>shuffle</code> 是否打乱文本顺序，如果相互独立的话  </li>
<li><code>random_state</code> 打乱顺序的随机种子</li>
<li><code>remove</code> 储存停用词的元组，例如标题引用</li>
<li><code>download_if_missing</code> 如果数据丢失则下载  </li>
</ul>
<h3 id="提取feature"><a href="#提取feature" class="headerlink" title="提取feature"></a>提取feature</h3><p>特征提取为数据挖掘任务最重要的环节之一，只有先提取出有效地特征，才能借助数据挖掘的力量找到问题的答案。常用的特征提取手段如下：</p>
<ul>
<li>DictVectorizer 将dict类型的list数据，转换成numpy array</li>
<li>FeatureHasher 特征哈希，相当于一种降维技巧</li>
<li>image 图像相关的特征提取</li>
<li>text 文本相关的特征提取  </li>
</ul>
<ol>
<li><p>text.CountVectorizer 将文本转换为每个词出现的个数的向量,总之用于标记和计数  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer       </span><br><span class="line">vectorizer = CountVectorizer(min_df=<span class="number">1</span>)</span><br><span class="line">vectorizer</span><br><span class="line">  CountVectorizer(analyzer=...<span class="string">'word'</span>, binary=<span class="keyword">False</span>, charset=<span class="keyword">None</span>,</span><br><span class="line">  charset_error=<span class="keyword">None</span>, decode_error=...<span class="string">'strict'</span>,</span><br><span class="line">  dtype=&lt;... <span class="string">'numpy.int64'</span>&gt;, encoding=...<span class="string">'utf-8'</span>, input=...<span class="string">'content'</span>,</span><br><span class="line">  lowercase=<span class="keyword">True</span>, max_df=<span class="number">1.0</span>, max_features=<span class="keyword">None</span>, min_df=<span class="number">1</span>,</span><br><span class="line">  ngram_range=(<span class="number">1</span>, <span class="number">1</span>), preprocessor=<span class="keyword">None</span>, stop_words=<span class="keyword">None</span>,</span><br><span class="line">  strip_accents=<span class="keyword">None</span>, token_pattern=...<span class="string">'(?u)\\b\\w\\w+\\b'</span>,</span><br><span class="line">  tokenizer=<span class="keyword">None</span>, vocabulary=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p> 通过计数，来将一个文档转化为向量，即单词矩阵(a matrix of token counts)<br> <code>CountVectorizer()</code>方法的参数：</p>
<ul>
<li>input: string{‘filename’,’file’,’content’} 待分析的文件名，可读的文件，或者可以直接分析的字符串或字节项  </li>
<li>encoding 解码格式 默认 utf-8</li>
<li>decode_error: {‘strict’,’ignore’,’replace’} 解码错误之后的处理方式，默认 strict</li>
<li>strip_accents: {‘ascii’,’unicode’,None} 去除特殊字符，使用ascii更快，unicode更慢但可以对任何字符使用</li>
<li>analyzer: string,{‘word’,’char’,’char_wb’} or callable 是否将特征转化为word（单词），char（n-grams，一种简化单词的方式）或者char_wb（将单词表示成n-grams的形式）</li>
<li>preprcessor: callable or None(default) 改写预处理函数，同时保留标记和n-grams处理过程</li>
<li>tokenizer: callable or None(default) 改写字符串标记步骤，同时保留预处理和n-grams步骤</li>
<li>ngram-range: tuple(min_n,max_n) 在[min_n,max_n]范围内的n值会被用于n-grams提取</li>
<li>stop_words: string {‘english’},list,or None(default) 停用词，使用英文停用词列表，或者使用自己的字符串作为停用词列表，或者不使用停用词，这时可以将max_df设置到[0.7,1.0],自动检验和过滤基于其语料库的停用词</li>
<li>lowercase: boolean, default True  将所有字符转换为小写  </li>
<li>token_pattern string 构成token的正则表达式，在tokenize == ‘word’ 时启用。默认的正则式为选择两个或两个以上的字符(标点符号是被忽略的，仅当做token的分割)</li>
<li>max_df float in range[0.0，1.0] or int, optional, 1 by default 构建忽略词的字典的时候，这些忽略词要求要严格的低于给定的阈值。这个值在书中称之为截至值，如果是浮点数，那么代表的是文档中的比例，如果是整数那么就是绝对计数值。如果字典不是空的话，这个参数被忽略</li>
<li>max_features optional, None by default 非None时，构建词典的时候仅仅考虑语料库里词频最高的那些特征，如果词典非空，那么这个参数被忽略</li>
<li>vocabulary Mapping or iterable, optional 词作为key，value在特征矩阵中可以索引；或者是一个单词的迭代；如果没有给定，则在输入的文档库中选择</li>
<li>binary boolean, False by default 如果为真，则所有非零的计数被置为1.这对离散的只针对二值事件而非整数计数的概率模型非常有用  </li>
<li>dtype: type, optional 通过fit_transform() or transform() 处理之后返回矩阵的类型.</li>
</ul>
</li>
<li><p>text.TfidfVectorizer 将文本转换为tfidf值的向量<br>为了重新计算特征的计数权重，过滤掉一些常见的且没有实际意义的信息，转化为适合分类器使用的浮点值，我们使用tf-idf转换。词频(term frequency, TF)指某一个给定的词语在该文件中出现的频率，这个数字通常被归一化，为了防止其偏向长的文件。逆向文件频率(inverse document frequency, IDF)是一个词语普遍重要性的度量，主要思想是：如果包含词条t的文档越少，那么IDF越大，说明词条具有更强的类别区分能力  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">transformer</span><br><span class="line"></span><br><span class="line">TfidfTransformer(norm=...<span class="string">'l2'</span>, smooth_idf=<span class="keyword">True</span>, sublinear_tf=<span class="keyword">False</span>,</span><br><span class="line">                 use_idf=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p><code>TfidfTransformer()</code>方法的参数：</p>
<ul>
<li>norm: ‘l1’,’l2’ or None, optional  norm参数用来正则化（归一化）词条向量，none则不进行正则化（归一化）（L1正则化和L2正则化可以看做是损失函数的惩罚项）</li>
<li>user_idf: boolean,default=True 使用逆向文件频率(idf)来进行加权</li>
<li>smooth_idf: boolean,default=Ture 平滑idf权重</li>
<li>sublinear_tf :booelean, default=False 使用亚曲线，例如使用<code>1 + log(tf)</code>来代替<code>tf</code>  </li>
<li><p>text.HashingVectorizer 文本的特征哈希<br>使用Hash Trick算法，将文本的特征进行降低。在降为之后，我们就无法解释特征向量中每一个维度的意义了，因此hash trick的解释性不强。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HashingVectorizer(input=<span class="string">'content'</span>, encoding=<span class="string">'utf-8'</span>, decode_error=<span class="string">'strict'</span>, strip_accents=<span class="keyword">None</span>, lowercase=<span class="keyword">True</span>, preprocessor=<span class="keyword">None</span>, tokenizer=<span class="keyword">None</span>, stop_words=<span class="keyword">None</span>, token_pattern=<span class="string">'(?u)\b\w\w+\b'</span>, ngram_range=(<span class="number">1</span>, <span class="number">1</span>), analyzer=<span class="string">'word'</span>, n_features=<span class="number">1048576</span>, binary=<span class="keyword">False</span>, norm=<span class="string">'l2'</span>, alternate_sign=<span class="keyword">True</span>, non_negative=<span class="keyword">False</span>, dtype=&lt;<span class="class"><span class="keyword">class</span> '<span class="title">numpy</span>.<span class="title">float64</span>'&gt;)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用HashingVectorizer，规定feature个数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> HashingVectorizer</span><br><span class="line">vectorizer = HashingVectorizer(stop_words = <span class="string">'english'</span>, non_negative = <span class="keyword">True</span>,</span><br><span class="line">    n_features = <span class="number">10000</span>)</span><br><span class="line">fea_train = vectorizer.fit_transform(newsgroup_train.data)</span><br><span class="line">fea_test = vectorizer.fit_transform(newsgroup_test.data)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'size of fea_train:'</span> + repr(fea_train.shape))</span><br><span class="line">print(<span class="string">'size of fea_test:'</span> + repr(fea_test.shape))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'The average feature sparsity is &#123;0:.3f&#125;%'</span>.format(</span><br><span class="line">fea_train.nnz/float(fea_train.shape[<span class="number">0</span>] * fea_train.shape[<span class="number">1</span>]) * <span class="number">100</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>HashingVectorizer()</code>方法的参数：</p>
<ul>
<li>input:  </li>
<li>encoding</li>
<li>decode_error</li>
<li>strip_accents</li>
<li>analyzer</li>
<li>preprocessor<br>tokenizer</li>
<li>ngram_range</li>
<li>stop_words</li>
<li>lowercase</li>
<li>token_pattern</li>
<li>n_features integer,default = (2 ** 20) 选择特征的数量。这个值太小的话会导致哈希碰撞(hash collisions)，太大的话起不到降维的效果(cause larger coefficient dimensions in linear leariners)</li>
<li>norm  ‘l1’, ‘l2’ or None, optional 正则化向量的方法</li>
<li>binary</li>
<li>dtype</li>
<li>alternate_sign boolean, optional, default True  <code>When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.</code></li>
<li><p>non_negative  boolean, optional, default False 对返回的特征矩阵的值取绝对值，这样会 <code>significantly reduces the inner product preservation property</code></p>
<p>其他参数的用法和<code>CountVectorizer()</code>一致  </p>
</li>
</ul>
</li>
</ol>
<h3 id="分类训练"><a href="#分类训练" class="headerlink" title="分类训练"></a>分类训练</h3><p>具体实现时用到了三种模型进行分类   </p>
<h5 id="Naive-Bayes-朴素贝叶斯法"><a href="#Naive-Bayes-朴素贝叶斯法" class="headerlink" title="Naive Bayes  朴素贝叶斯法"></a>Naive Bayes  朴素贝叶斯法</h5>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'*************************\nNaive Bayes\n*************************'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB <span class="comment"># 先验为多项式的朴素贝叶斯</span></span><br><span class="line">clf = MultinomialNB(alpha = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 拟合数据</span></span><br><span class="line">clf.fit(tfidf_train_2, newsgroup_train.target)</span><br><span class="line"><span class="comment"># 预测测试集</span></span><br><span class="line">pred = clf.predict(tfidf_test_2)</span><br></pre></td></tr></table></figure>
<p>  <code>MultinomialNB()</code> 先验为多项式的朴素贝叶斯，  </p>
<script type="math/tex; mode=display">P(X_j = x_{jl}|Y = C_k) = \frac{x_{jl} + \lambda}{m_k + n \lambda}</script><p>  参数如下：</p>
<ul>
<li>alpha: 即上述公式中的 $\lambda$ ,一般默认为1，称做拉普拉斯平滑。如果发现拟合不好，需要调优时，可以选择稍微大于1或者稍微小于1的数。</li>
<li>fit_prior boolean，表示是否考虑先验概率。如果是false，则所有的样品类别输出都有相容的类别先验概率。否则用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior，让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率 $P(Y = C_k) = m_k/m$ 。其中 $m$ 为训练样本总数量， $m_k$ 为第k类别的训练集样品数。  总结如下：  </li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">fit_prior</th>
<th style="text-align:left">class_prior</th>
<th style="text-align:left">最终先验概率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">false</td>
<td style="text-align:left">填不填没有意义</td>
<td style="text-align:left">$P(Y = C_k) = 1/k$</td>
</tr>
<tr>
<td style="text-align:left">true</td>
<td style="text-align:left">不填</td>
<td style="text-align:left">$P(Y = C_k) = m_k/m$</td>
</tr>
<tr>
<td style="text-align:left">true</td>
<td style="text-align:left">填</td>
<td style="text-align:left">$P(Y = C_k) = class_prior$</td>
</tr>
</tbody>
</table>
</div>
<p>  来自<a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">scikit-learn 朴素贝叶斯类库使用小结</a>  </p>
<h5 id="KNN-k近邻算法"><a href="#KNN-k近邻算法" class="headerlink" title="KNN  k近邻算法"></a>KNN  k近邻算法</h5>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">print(<span class="string">'*************************\nKNN\n*************************'</span>)</span><br><span class="line">knnclf = KNeighborsClassifier()<span class="comment">#default with k=5</span></span><br><span class="line">knnclf.fit(tfidf_train_2,newsgroup_train.target)</span><br><span class="line">pred = knnclf.predict(tfidf_test_2)</span><br></pre></td></tr></table></figure>
<p>  <code>KNeighborsClassifier()</code>方法参数:</p>
<ul>
<li>n_neighbors:int,optional default=5 k近邻算法中k的值，或者说k近邻搜索中选取最近的点的个数</li>
<li>weights: str or callable, optional default = ‘uniform’<ul>
<li>‘uniform’ 统一权重</li>
<li>‘distance’ 点的权重与距离成反比</li>
<li>[callable] 使用用户自定义的函数</li>
</ul>
</li>
<li>algorithm:  {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional 实现算法，BallTree， KDTree， brute-force search. ‘auto’会根据传入到fit方法中的值来决定最合适的实现算法</li>
<li>leaf_size: int optional default = 30 传入BallTree或KDTree中的叶子结点个数   </li>
<li>p:  integer,optional default=2 距离度量，p=1时是曼哈顿距离(Manhattan distance)，p=2时是欧式距离(Euclidean distance)  </li>
<li>metric: string or callable, default ‘minkowski’ 计算距离的公式，默认为Minkowski距离  </li>
<li>metric_params: dict,optional default=None  上面自己定义的距离度量函数的参数</li>
<li>n_jobs: int, optional default = 1 近邻搜索时的并行工作数量 如果设置为-1，则设置为cpu的核心数   <h5 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM  支持向量机"></a>SVM  支持向量机</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">print(<span class="string">'*************************\nSVM\n*************************'</span>)</span><br><span class="line">svclf = SVC(kernel = <span class="string">'linear'</span>)</span><br><span class="line">svclf.fit(tfidf_train_2,newsgroup_train.target)</span><br><span class="line">pred = svclf.predict(tfidf_test_2)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>SVC()</code>这个函数是基于<code>libsvm</code>实现的，在参数设置上也有很多相似的地方。         </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">'rbf'</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="keyword">True</span>, probability=<span class="keyword">False</span>,tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, verbose=<span class="keyword">False</span>, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="keyword">None</span>,random_state=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数:  </p>
<ul>
<li>C: float, optional default=1.0 C-SVC的惩罚参数 C越大，相对于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。</li>
<li>kernel: string, optional {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’, callable}，default = ‘rbf’  核函数，可选择 线性，多项式，rbf函数，sigmoid函数</li>
<li>degree: integer, optional default=3 多项式poly函数的维度  </li>
<li>gamma: float, optional default=0.0  ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features  </li>
<li>coef0: float, optional default=True 核函数的常数项，应用于’poly’和’sigmoid’</li>
<li>probability: boolean, optional default=True  是否使用概率估计(probability estimates), 这必须在fit之前调用，bong将减低fit方法的速度</li>
<li>shrinking: boolean, optional default=true  是否采用shrinking heuristic方法</li>
<li>tol: float, optional default = 1e-3  停止训练的误差值大小</li>
<li>cache_size: float, optional   核函数的cache缓存大小  </li>
<li>class_weight: {dict, ‘balanced’}, optional  类别的权重，字典形式传递</li>
<li>verbose: bool, default = False  允许冗余输出(verbose output),只使用与单线程的方法，在多线程中可能出错</li>
<li>max_iter: int, optional, default=-1 最大迭代次数， -1为无限制</li>
<li>decision_function_shape: ‘ovo’, ‘ovr’, default = ‘ovr’  </li>
<li>random_state: int，RandomState instance or None, optional  default=None 数据洗牌时的种子<br>来自<a href="https://blog.csdn.net/szlcw1/article/details/52336824" target="_blank" rel="noopener">sklearn.svm.SVC 参数说明</a></li>
</ul>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>这边使用的是KMeans聚类算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">print(<span class="string">'*************************\nKMeans\n*************************'</span>)</span><br><span class="line">pred = KMeans(n_clusters=<span class="number">5</span>)</span><br><span class="line">pred.fit(tfidf_test_2)</span><br><span class="line">calculate_result(newsgroup_test.target,pred.labels_)</span><br></pre></td></tr></table></figure>
<p>关于<code>KMeans()</code>方法的参数:</p>
<ul>
<li>n_clusters: int,optional, default=8 数据分组数量，或者生成质心的数量  </li>
<li>init : {‘k-means++’, ‘random’ or an ndarray} default=’k-means++’  </li>
<li>n_init : int, default=10</li>
<li>max_iter : int, default: 300 一次运行中KMeans算法的最大迭代次数  </li>
<li>tol : float, default=1e-4 停止训练的误差值大小  </li>
<li>precompute_distances : {‘auto’, True, False} 预先计算距离（更快但消耗内存）<ul>
<li>auto n_samples * n_clusters &gt; 12 million则不使用预先计算</li>
<li>True</li>
<li>False</li>
</ul>
</li>
<li>verbose : int, default=0 冗余模式(verbosity mode)</li>
<li>random_state : int, RandomState instance or None, optional, default=None 数据洗牌时的种子</li>
<li>copy_x : boolean, default=True  是否复制原始数据 True复制原始数据，因此不修改原始数据，False则使用修改后的原始数据。</li>
<li>n_jobs : int 并行数量 -1则与cpu核心数相同</li>
<li>algorithm : “auto”, “full” or “elkan”, default=”auto” 选择实现算法  </li>
</ul>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>模型的评估一般使用PRF（精确率，召回率，F1值）和Acc值（准确值）来评估，我们定义了<code>calculate_result()</code>来进行评估。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_result</span><span class="params">(actual, pred)</span>:</span></span><br><span class="line">  m_precision = metrics.precision_score(actual,pred, average=<span class="string">'weighted'</span>)</span><br><span class="line">  m_recall = metrics.recall_score(actual, pred, average=<span class="string">'weighted'</span>)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">'predict info:'</span>)</span><br><span class="line">  print(<span class="string">'precision:&#123;0:.3f&#125;'</span>.format(m_precision))</span><br><span class="line">  print(<span class="string">'recall:&#123;0:0.3f&#125;'</span>.format(m_recall))</span><br><span class="line">  print(<span class="string">'f1-score:&#123;0:.3f&#125;'</span>.format(metrics.f1_score(actual,pred,average=<span class="string">'weighted'</span>)))</span><br></pre></td></tr></table></figure>
<p>其中的定义</p>
<ol>
<li>TP，True Positive 做出pos判断且判断正确</li>
<li>FP，False Positive 做出pos判断且判断错误</li>
<li>TN，True Negative 做出neg判断且判断正确</li>
<li>FN，False Negative 做出neg判断且判断错误</li>
</ol>
<ul>
<li>precision 精确率 precision = TP / (TP + FP) 即所有做出pos判断的判定中判断正确的概率</li>
<li>recall 召回率  recall = TP / (TP + FN) 即所有应该被判断为pos的对象中被判断为pos的概率</li>
<li>accuracy 准确度 accuracy = (TP + TN) / (TP + FP + TN + FN) 即所有判断中，判断正确的概率</li>
<li>F1 score = 2 <em> P </em> R / (P + R) 其中P为precision, R为 recall</li>
</ul>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>最后，我选择<code>fetch_20newsgroups_vectorized()</code>方法提取的特征来训练分类聚类模型，最后的输出如下</p>
<pre><code class="lang-text">*************************
Naive Bayes
*************************
predict info:
precision:0.768
recall:0.766
f1-score:0.763
*************************
KNN
*************************
predict info:
precision:0.650
recall:0.641
f1-score:0.642
*************************
SVM
*************************
predict info:
precision:0.822
recall:0.818
f1-score:0.818
*************************
KMeans
*************************
predict info:
precision:0.077
recall:0.153
f1-score:0.093
</code></pre>
<p>不知为何聚类结果比原作者运行时的结果评分低了很多  </p>
<pre><code class="lang-text">*************************
KMeans
*************************
predict info:
precision:0.289
recall:0.313
f1-score:0.266
</code></pre>
<p>附上我这边能够运行的<a href="http://oqvbarc9z.bkt.clouddn.com/test_textanalyse.py" target="_blank" rel="noopener">代码</a><br>Environment：Python 3.6 + Scipy （scikit-learn）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://channel960608.github.io/2018/04/11/通过20newsgroups案例进行Sklearn入门学习/" data-id="ck43xmmod000huhznxiu24sa3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/12/13/寻找陈络月/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Search for Chenluo Month
        
      </div>
    </a>
  
  
    <a href="/2018/04/08/k近邻算法中kd树的实现/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">k近邻算法中kd树的实现</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
<!--高速版，加载速度快，使用前需测试页面的兼容性-->
<div id="SOHUCS" sid="通过20newsgroups案例进行Sklearn入门学习" style="padding: 0px 30px 0px 46px;"></div>
<script>
  (function(){
    var appid = 'cyt29gnpS',
    conf = 'ab6fb0a4df25f655315dc29bcd3837b3';
    var doc = document,
    s = doc.createElement('script'),
    h = doc.getElementsByTagName('head')[0] || doc.head || doc.documentElement;
    s.type = 'text/javascript';
    s.charset = 'utf-8';
    s.src =  'http://assets.changyan.sohu.com/upload/changyan.js?conf='+ conf +'&appid=' + appid;
    h.insertBefore(s,h.firstChild);
    window.SCS_NO_IFRAME = true;
  })()
</script>
  </section>
  </section>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a href="/" class="logo">Coder Am I</a>
      &copy; 2019 Channel<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    

<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollLoading.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>